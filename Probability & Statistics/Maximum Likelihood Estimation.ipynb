{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135f34fe-e59d-4fe9-8735-aa5dce20a5b5",
   "metadata": {},
   "source": [
    "#### MLE for a Coin Flip (Bernoulli Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90e065e7-5389-48f1-9d28-528fe147892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimate for Coin Flip Probability (theta): 0.6000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Coin flip results: 1 = Heads, 0 = Tails\n",
    "coin_flips = np.array([1, 0, 1, 1, 0, 1, 1, 0, 1, 0])\n",
    "\n",
    "# Define the Negative Log-Likelihood function for Bernoulli\n",
    "def neg_log_likelihood(theta, data):\n",
    "    n_heads = np.sum(data)\n",
    "    n_tails = len(data) - n_heads\n",
    "    return -(n_heads * np.log(theta) + n_tails * np.log(1 - theta))\n",
    "\n",
    "# Find theta that minimizes the negative log-likelihood\n",
    "result = minimize(lambda theta: neg_log_likelihood(theta, coin_flips),\n",
    "                  x0=np.array([0.5]),  # Initial guess\n",
    "                  bounds=[(0.01, 0.99)])  # Keep theta in valid probability range\n",
    "\n",
    "theta_mle = result.x[0]\n",
    "print(f\"MLE Estimate for Coin Flip Probability (theta): {theta_mle:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396982e-0ed6-4aa2-bd1a-f87b2a945353",
   "metadata": {},
   "source": [
    "#### MLE for Normal Distribution (Estimating Mean & Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36a5376-2183-4281-889f-5fbd861052ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimate of Mean: 4.7923\n",
      "MLE Estimate of Standard Deviation: 1.8072\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(loc=5, scale=2, size=100)  # True mu=5, sigma=2\n",
    "\n",
    "# Compute MLE estimates\n",
    "mu_mle = np.mean(data)\n",
    "sigma_mle = np.sqrt(np.mean((data - mu_mle) ** 2))  # No Bessel's correction\n",
    "\n",
    "print(f\"MLE Estimate of Mean: {mu_mle:.4f}\")\n",
    "print(f\"MLE Estimate of Standard Deviation: {sigma_mle:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518170f8-9f99-4b17-a32b-65de83f56dbd",
   "metadata": {},
   "source": [
    "#### MLE for Logistic Regression (Gradient Ascent Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e1c8f4-70ba-4b45-aa2e-255520417461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimate for Logistic Regression Weights: [ 16.92904065 -16.39472045  18.02675548]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.special import expit  # Sigmoid function\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_classification(n_samples=100, n_features=2, \n",
    "                           n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "X = np.c_[np.ones(X.shape[0]), X]  # Add bias term\n",
    "\n",
    "# Define negative log-likelihood function\n",
    "def neg_log_likelihood(beta, X, y):\n",
    "    logits = X @ beta  # Linear combination\n",
    "    probs = expit(logits)  # Sigmoid activation\n",
    "    return -np.sum(y * np.log(probs) + (1 - y) * np.log(1 - probs))\n",
    "\n",
    "# Initialize weights\n",
    "beta_init = np.zeros(X.shape[1])\n",
    "\n",
    "# Optimize using MLE\n",
    "result = minimize(neg_log_likelihood, beta_init, args=(X, y))\n",
    "\n",
    "beta_mle = result.x\n",
    "print(f\"MLE Estimate for Logistic Regression Weights: {beta_mle}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c21dd-eb31-4699-bde4-89a930c66d56",
   "metadata": {},
   "source": [
    "#### MLE for Naive Bayes Classifier (Text Classification Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75948042-d9ae-423e-baea-7ce4f53a66e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P('win' | spam) = 0.1818\n",
      "P('win' | ham) = 0.1250\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Sample text dataset\n",
    "documents = [\n",
    "    (\"spam\", \"Win money now\"),\n",
    "    (\"spam\", \"Claim your free prize\"),\n",
    "    (\"ham\", \"Meeting at 5pm\"),\n",
    "    (\"ham\", \"Project deadline is near\"),\n",
    "    (\"spam\", \"Congratulations, you won\"),\n",
    "]\n",
    "\n",
    "# Preprocess\n",
    "word_counts = {\"spam\": Counter(), \"ham\": Counter()}\n",
    "class_counts = Counter()\n",
    "\n",
    "for label, text in documents:\n",
    "    words = text.lower().split()\n",
    "    word_counts[label].update(words)\n",
    "    class_counts[label] += 1\n",
    "\n",
    "# Compute MLE P(word | class)\n",
    "def mle_word_prob(word, class_label):\n",
    "    return (word_counts[class_label][word] + 1) / (sum(word_counts[class_label].values()) + 1)\n",
    "\n",
    "# Example: Compute P(\"win\" | spam)\n",
    "print(f\"P('win' | spam) = {mle_word_prob('win', 'spam'):.4f}\")\n",
    "print(f\"P('win' | ham) = {mle_word_prob('win', 'ham'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89f0d2-fb6a-481a-9657-38ac68cd6a82",
   "metadata": {},
   "source": [
    "#### MLE for Gaussian Mixture Model (GMM) using Expectation-Maximization (EM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31e69bf9-8d72-4af6-99e6-625398123293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Means: [10.02016994  4.89566882]\n",
      "Estimated Variances: [0.90785176 0.81747606]\n",
      "Estimated Weights: [0.50025557 0.49974443]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Generate synthetic data from two Gaussians\n",
    "np.random.seed(42)\n",
    "data1 = np.random.normal(5, 1, 100)  # Cluster 1\n",
    "data2 = np.random.normal(10, 1, 100)  # Cluster 2\n",
    "data = np.concatenate([data1, data2]).reshape(-1, 1)\n",
    "\n",
    "# Fit GMM with MLE\n",
    "gmm = GaussianMixture(n_components=2, max_iter=100)\n",
    "gmm.fit(data)\n",
    "\n",
    "# Print estimated parameters\n",
    "print(f\"Estimated Means: {gmm.means_.flatten()}\")\n",
    "print(f\"Estimated Variances: {gmm.covariances_.flatten()}\")\n",
    "print(f\"Estimated Weights: {gmm.weights_.flatten()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112d70b-32b5-4369-936c-d67c2be30cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
